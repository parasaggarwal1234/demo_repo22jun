{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ff3ed3",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "1) Decision-tree algorithm falls under the category of supervised learning algorithms. <br>\n",
    "2) Decision Tress is used for both classification and regression. <br>\n",
    "3) <b>A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).</b><br>\n",
    "4) The decision tree from the name itself signifies that it is used for making decisions from the given dataset. The concept behind the decision tree is that it helps to select appropriate features for splitting the tree into subparts.<br>\n",
    "5) It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.<br>\n",
    "6) Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label(target variable) and attributes are represented on the internal node of the tree.<br>\n",
    "\n",
    "#### Example\n",
    "We have 100 rows of data with<br>\n",
    "x = 'Age', 'Gender', 'BMI','Body_wt', 'Blood_Glucose_Level'<br>\n",
    "y = 0 or 1 [0 = Non-diabteic, 1 = Diabetic]<br>\n",
    "65 are diabetic and 35 are non-diabetic\n",
    "\n",
    "<img src=\"dt1.png\">\n",
    "\n",
    "\n",
    "After Splitting\n",
    "<img src=\"dt_tree_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8b204",
   "metadata": {},
   "source": [
    "### How to decide the feature for splitting at root node or internal nodes?\n",
    "Statistical measures like <b>Entropy,Information Gain and Gini Index</b> are used to decide the columns for spliiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0543fd",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithms\n",
    "\n",
    "<b>ID3 (Iterative Dichotomiser 3)</b><br>\n",
    "ID3 decision tree algorithm uses Information Gain to decide the splitting parameters. In order to measure how much information we gain, we can use entropy to calculate the homogeneity of a sample.\n",
    "\n",
    "<b>CART (Classification and Regression Tree)</b><br>\n",
    "CART uses the Gini method to create split points including Gini Index (Gini Impurity) and Gini Gain.\n",
    "\n",
    "<b>CHAID (Chi-Square Automatic Interaction Detection)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd6438",
   "metadata": {},
   "source": [
    "### Limitations of Decision Tree\n",
    "\n",
    "1) It suffers from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f769a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
