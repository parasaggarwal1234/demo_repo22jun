{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a799403c",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "1) It stands for Xtreme Gradient Boosting.<br>\n",
    "2) Is one of the well-known state of the art techniques(ensemble) having enhanced performance and speed in tree-based (sequential decision trees) machine learning algorithms. <br>\n",
    "3) In Boosting technique the errors made by previous models are tried to be corrected by succeeding models by adding some weights to the models. <br>\n",
    "\n",
    "### Features of XGBoost\n",
    "\n",
    "1) Can be run on both single and distributed systems(Hadoop, Spark).<br>\n",
    "2) XGBoost is used in supervised learning(regression and classification problems).<br>\n",
    "3) Supports parallel processing - Use all CPU cores in the system.<br>\n",
    "4) Efficient memory management for large datasets exceeding RAM.<br>\n",
    "5) Has a variety of regularizations which helps in reducing overfitting.<br>\n",
    "6) Auto tree pruning – Decision tree will not grow further after certain limits internally.<br>\n",
    "7) Can handle missing values.<br>\n",
    "8) Has inbuilt Cross-Validation.<br>\n",
    "9) Takes care of outliers to some extent.<br>\n",
    "10) XGBoost can run distributed thanks to distributed servers and clusters like Hadoop and Spark, so you can process enormous amounts of data. It’s also available for many programming languages like C++, JAVA, Python, and Julia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9829d",
   "metadata": {},
   "source": [
    "### Steps in XGBoost\n",
    "\n",
    "Residuals - Difference between actula and predicted value.\n",
    "\n",
    "<b>1) Model computes initial residuals </b><br>\n",
    "a) For Classification Probability to compute residuals for base model is assumed to be 0.5<br>\n",
    "b) For Regression model probability to compute residuals is avg value of the target variable.<br>\n",
    "\n",
    "<b>2) Model Computes the similarity Score</b><br>\n",
    "a) For Regression<br>\n",
    "\n",
    "$ Similarity Score(SS)  = \\frac {(\\sum Residuals)^2}  {NumberOfResiduals  +  \\lambda}  $\n",
    "\n",
    "b) For Classification<br>\n",
    "$ Similarity Score (SS) = \\frac {(\\sum Residuals)^2} {\\sum [PreviousProbability*(1-PreviousProbability)] + \\lambda }  $\n",
    "\n",
    "where λ = Regularization Parameter\n",
    "\n",
    "\n",
    "<b>3) Model computes Gain, the formula for which is same for both Regression and Classification</b>\n",
    "\n",
    "$ Gain = {LeftNode}_{SS}  + {RightNode}_{SS} - {RootNode}_{SS} $\n",
    "\n",
    "\n",
    "<b>4) Branches are purned based on parameter $ \\gamma $</b>\n",
    "\n",
    "Rule<br>\n",
    "\n",
    "$ Gain - \\gamma $ = Negative => Remove the branch\n",
    "\n",
    "$ Gain - \\gamma $ = Positive => Don't Remove the branch\n",
    "\n",
    "\n",
    "<b>5) Compute Output Value(OV)</b><br>\n",
    "a) For Regression<br>    \n",
    "$ Output Value  = \\frac {\\sum Residuals}  {NumberOfResiduals  +  \\lambda}  $    \n",
    "    \n",
    "b) For Classification<br>    \n",
    "$ Output Value  = \\frac {\\sum Residuals} {\\sum [PreviousProbability * (1-PreviousProbability)] + \\lambda }  $        \n",
    "    \n",
    "<b>6) Compute New Predicted Value </b><br>\n",
    "a) For Regression    \n",
    "\n",
    "$ {Predicted Value}_{new} = OriginalPrediction +   \\epsilon * Output Value $\n",
    "\n",
    "b) For Classification\n",
    "\n",
    "\n",
    "$ {Predicted Value}_{new} = \\log(odds) +  \\epsilon * Output Value $\n",
    "\n",
    "where log(odds) = log(previous_prob/1-previous_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef88fd7",
   "metadata": {},
   "source": [
    "#### XGboost Parameters\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7988896",
   "metadata": {},
   "source": [
    "#### How to Install XGBoost\n",
    "\n",
    "1) Write the following in CMD<br>\n",
    "pip install xgboost<br>\n",
    "\n",
    "2) In Jupyter, write the following<br>\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7772ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a631e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
