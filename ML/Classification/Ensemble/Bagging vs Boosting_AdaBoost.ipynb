{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825e1c82",
   "metadata": {},
   "source": [
    "### Ensemble Leanring\n",
    "1) Ensemble learning combines several base algorithms to form one optimized predictive algorithm.<br>\n",
    "\n",
    "2) A typical Decision Tree for classification takes several factors, turns them into rule questions, and given each factor, either makes a decision or considers another factor.\n",
    "\n",
    "3) Instead of relying on one Decision Tree to make the right call, Ensemble Methods take several different trees and aggregate them into one final, strong predictor.\n",
    "\n",
    "### Types of Ensemble Learning Techniques\n",
    "\n",
    "### 1) Bagging\n",
    "a) It works on paraller processing technique. Ex - RandomForestClassifier.<br>\n",
    "b) Each tree has equal say in the final model.<br>\n",
    "c) Each tree is fully frown as a part of individual model.(ofcouse subject to usage of max_depth,min_sample_split and other hyperparamters)<br>\n",
    "    \n",
    "    \n",
    "### 2) Boosting\n",
    "a) It works on Sequential Porcessing. Eg- AdaBoost<br>\n",
    "b) All models dont have equal say (weightage) in the final model.<br>\n",
    "c) Decision Stumps are used (in AdaBoost). Each stump consist of Decision nodes and leaf nodes.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0a2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67603af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18b0feed",
   "metadata": {},
   "source": [
    "### AdaBoost (Adaptive Boost)\n",
    "\n",
    "1) AdaBoost or Adaptive Boosting is one of the ensemble boosting classifier proposed by Yoav  and Robert in 1996.\n",
    "\n",
    "2) It combines multiple weak classifiers to increase the accuracy of classifiers.\n",
    "\n",
    "3) AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier.\n",
    "\n",
    "4) The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations.\n",
    "\n",
    "5) AdaBoost should meet two conditions:<br>\n",
    "a) The classifier should be trained on various weighed training examples.<br>\n",
    "b) In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.\n",
    "\n",
    "6) To build a AdaBoost classifier, imagine that as a first base classifier we train a Decision Tree algorithm to make predictions on our training data.\n",
    "\n",
    "7) Now, following the methodology of AdaBoost, the weight of the misclassified training instances is increased.\n",
    "\n",
    "8) The second classifier is trained and acknowledges the updated weights and it repeats the procedure over and over again.\n",
    "\n",
    "9) At the end of every model prediction we end up boosting the weights of the misclassified instances so that the next model does a better job on them, and so on.\n",
    "\n",
    "10) AdaBoost adds predictors to the ensemble gradually making it better.\n",
    "\n",
    "11) Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20991ccf",
   "metadata": {},
   "source": [
    "### Steps in AdaBoost\n",
    "\n",
    "1) Initially, all observations are given equal weights equal to 1/number of observations<br>\n",
    "2) A model is built on a subset of data.<br>\n",
    "3) Using this model, predictions are made on the whole dataset.<br>\n",
    "4) Errors are calculated by comparing the predictions and actual values.<br>\n",
    "5) While creating the next model, higher weights are given to the data points which were predicted incorrectly.<br>\n",
    "6) Weights can be determined using the error value. For instance,the higher the error the more is the weight assigned to the observation.<br>\n",
    "7) This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached<br>\n",
    "8) To classify, perform a \"vote\" across all of the learning algorithms you built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69cda2",
   "metadata": {},
   "source": [
    "### Hyperparameters of AdaBoost\n",
    "\n",
    "1) <b>base_estimator </b> - It is the learning algorithm to use to train the weak models. This will almost always not needed to be changed because by far the most common learner to use with AdaBoost is a decision tree – this parameter’s default argument.\n",
    "\n",
    "2) <b>n_estimators</b> It is the number of models to iteratively train.\n",
    "\n",
    "3) <b>learning_rate</b> - It is the contribution of each model to the weights and defaults to 1. Reducing the learning rate will mean the weights will be increased or decreased to a small degree, forcing the model train slower (but sometimes resulting in better performance scores).\n",
    "\n",
    "4) <b>algo</b> - SAMME, SAMME.R<br>\n",
    "These algorithms are adaptations of the main idea of Ababoost extending their functionality with multiclass capabilities.<br>\n",
    "The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5b5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
