{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5ac20a",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "When model performs well on the training data but comparatively poorly on the test data. This condition is referred to as overfitting.\n",
    "\n",
    "Training Score = 0.87467<br>\n",
    "Testing Score  = 0.61298"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faee75",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "1) It is a technique to reduce overfitting.<br>\n",
    "2) It is implemeneted by adding a penalty term to the model.<br>\n",
    "3) <b>Ridge, Lasso and ElasticNet regression</b> are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648d840",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 Regularization)\n",
    "1) In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients.<br>\n",
    "2) Ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity.<br>\n",
    "3) So ridge regression puts constraint on the coefficients (w). The penalty term (lambda) regularizes the coefficients such that if the coefficients take large values the optimization function is penalized<br>\n",
    "4) When λ → 0 , the cost function becomes similar to the linear regression cost function. So lower the constraint (low λ) on the features, the model will resemble linear regression model.\n",
    "<img src=\"ridge_reg1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8504cef",
   "metadata": {},
   "source": [
    "### Lasso Regression (L1 Regularization)\n",
    "1) Least absolute shrinkage and selection operator.<br>\n",
    "2) Just like Ridge regression cost function, for lambda =0, the equation above reduces to the equation of linear regresion<br>\n",
    "3) The only difference is instead of taking the square of the coefficients, magnitudes are taken into account.<br>\n",
    "4) This type of regularization (L1) can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.<br>\n",
    "\n",
    "<img src=\"lasso_reg1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1bb46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b51b79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda = alpha\n",
    "m1 = Ridge(alpha=0.1)\n",
    "m2 = Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17c078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
